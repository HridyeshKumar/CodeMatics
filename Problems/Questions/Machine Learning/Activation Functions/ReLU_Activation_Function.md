# ReLU Activation Function

The ReLU (Rectified Linear Unit) function is a popular activation function used in neural networks, particularly in deep learning models. It has gained widespread adoption due to its simplicity and effectiveness in training deep networks. Here's an introduction to the ReLU function:

## What is ReLU?
ReLU stands for Rectified Linear Unit. It is a piecewise linear function that outputs the input directly if it is positive; otherwise, it outputs zero. Mathematically, the ReLU function is defined as:


## Mathematical Representations of Activation Functions

### ReLU (Rectified Linear Unit)

The ReLU function is defined as:
$${ReLU}(x) = \max(0, x)$$
where $x$ is the input to the function.


ReLU(x) is the output of the function.

## Challenge
Create a ReLU function and plot its curve.